{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a739ab4-8117-451c-93bb-3075fa390df2",
   "metadata": {},
   "source": [
    "# Model Predictive Control and Neural Network-based control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a92877-71e5-4397-87be-400cc664b286",
   "metadata": {},
   "source": [
    "The goal of this exercise is to implement two controllers for the `Pendulum-v1` environment of the _Gymnasium_ library: a model-based controller and a model-free, neural network-based controller. Please **read carefully** the [documentation](https://gymnasium.farama.org/environments/classic_control/pendulum/) of environment before starting (focus on the state variables and controls). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65567dd5-8ee2-4d5e-a59d-168211b1cc42",
   "metadata": {},
   "source": [
    "## Part 1: Model Predictive Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8180829-7622-4339-9984-d4481a35cbfd",
   "metadata": {},
   "source": [
    "Implement a model-based controller that uses the Model Predictive Control (MPC) theory (see the slides in the _Genetic Algorithm_ set in the repo of the course) to **stabilize the pendulum in its upright position** ($\\theta = 0$, $\\omega = 0$). Set the **gravity equal to 9.81** using the `g` argument of the `Pendulum-v1` environment. In general, you should follow these steps:\n",
    "\n",
    "1. Define the _cost_ function associated to the MPC using the **reward** of the environment. For the prediction of the future\n",
    "   states and rewards associated to a sequence of actions, another _Pendulum_ environment called `env_mpc` (separate from the one\n",
    "   `main_env` that the controller is interacting with) should be used.\n",
    "2. Define a `MPC` class compatible with `pygmo` that implements the optimization problem\n",
    "   that needs to be solved using Model Predictive Control to compute the optimal action\n",
    "   (the one that minimizes the cost defined at Step 1). Remember to appropriately set\n",
    "   the bounds for the controls.\n",
    "3. Implement the `get_best_mpc_action` to actually solve the MPC problem using `pygmo`.\n",
    "4. Define the function `play_game` to play a \"game\" using a controller chosen by the\n",
    "   user among the following: 1) MPC; 2) random; 3) Neural Network (see Part 2). The\n",
    "   _initial conditions_ for the angle and angular velocity should be randomly set within\n",
    "   the intervals -20/+20 degrees and -0.1/0.1 rad/s, respectively, using the\n",
    "   `env.unwrapped.state` variable. For the MPC controller, at each time step an\n",
    "   optimization problem must be solved using the function defined at Step 3.\n",
    "   The function should store and return the lists of _observations_ and corresponding\n",
    "   _controls_ and the _total score_ associated to the game. Each game lasts maximum 200 actions.\n",
    "5. Play _a few_ games with _random_ initial conditions (angle between -20 and +20 degrees and angular velocity between -0.1 and 0.1 rad/s) and compute the _average total score_. **You should get a total score above -10, at least in some games.** For one game, plot the angle and the angular velocity as a function of time, and the controls in a separate figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import pygmo as pg\n",
    "from torch import nn, tensor\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b74b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mpc(action_sequence, main_env, env_mpc):\n",
    "    # set initial state of the environment used for simulating MPC actions equal to the\n",
    "    # current state in the \"real\" environment\n",
    "    env_mpc.unwrapped.state = main_env.unwrapped.state\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd221075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pygmo problem class for MPC\n",
    "class MPC():\n",
    "    def __init__(self, main_env, env_mpc, control_horizon=10):\n",
    "        pass\n",
    "\n",
    "    def fitness(self, action_sequence):\n",
    "        pass\n",
    "\n",
    "    def get_bounds(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09899208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return best action by minimizing MPC cost using genetic algorithm\n",
    "def get_best_mpc_action(env, env_mpc):\n",
    "    # best_action = ... \n",
    "    # convert the results into a 1x1 numpy array to be compatible with the step method\n",
    "    # of the gym environment\n",
    "    return np.array([best_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e72d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game with specified controller\n",
    "def play_game(controller, nn_model=None):\n",
    "    env = gym.make('Pendulum-v1', g=9.81)  # Main environment\n",
    "    env.reset()\n",
    "    env_mpc = gym.make('Pendulum-v1', g=9.81) # Environment for MPC\n",
    "    env_mpc.reset()\n",
    "\n",
    "    # initial conditions\n",
    "    # ...\n",
    "\n",
    "    # set initial x, y and omega (angular velocity)\n",
    "    previous_obs = np.array([np.cos(initial_angle), np.sin(initial_angle), initial_angular_vel])\n",
    "\n",
    "    states = []\n",
    "    controls = []\n",
    "    total_score = 0\n",
    "\n",
    "    for _ in range(200):\n",
    "        pass\n",
    "\n",
    "    return states, controls, total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a7bc4-9cb9-45c9-8a95-45e915686c90",
   "metadata": {},
   "source": [
    "## Part 2: Neural Network controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53216e07-ff8f-43bf-9fe9-afbd8c6ff244",
   "metadata": {},
   "source": [
    "In this part, you will train a neural-network based controller based on the optimal control strategy found in Part 1. To this aim you should:\n",
    "1. Write a function `generate_training_data` to generates a training dataset by playing a certain number of games (suggested minimum\n",
    "   100) using the MPC controller implemented in Part 1 and providing as random initial conditions\n",
    "   an angle between -20 and 20 degrees and zero angular velocity. Each step of the game\n",
    "   corresponds to a sample of the training dataset, where $x$, $y$, $\\omega$ are the\n",
    "   features and the action chosen by the MPC controller is the label. Note: this step may be\n",
    "   _slow_. Finally, convert the dataset to `torch` tensors ($X$ and $y$) with float32 precision.\n",
    "2. Create a feedforward neural network implemented in `torch` that takes the current **observation** ($x$, $y$, $\\omega$) as an input and returns the **control** to be applied to the system. Make sure that the returned value is \"admissible\".\n",
    "3. **Train** and **select** the network (by exploring different _architectures_ and values for the _hyperparameters_).\n",
    "4. Play 2000 games with the controls given by the \"best\" network obtained in Step 3. Compute the average total score. **You should get an average score above -5.** Compare the average total score with that of a _random controller_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c07f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(num_games):\n",
    "    env = gym.make('Pendulum-v1', g=9.81)  # Main environment\n",
    "    env_mpc = gym.make('Pendulum-v1', g=9.81) # Environment for MPC\n",
    "\n",
    "    training_data = []\n",
    "\n",
    "    for i in range(num_games):\n",
    "        print(\"Playing game number \", i + 1)\n",
    "        env.reset()\n",
    "        env_mpc.reset()\n",
    "\n",
    "        # ...\n",
    "        # ...\n",
    "\n",
    "\n",
    "        # save only the actions of \"successful\" games\n",
    "        if total_score >= -0.8:\n",
    "            # ...\n",
    "        \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97adfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = generate_training_data(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tensor([i[0] for i in training_data], dtype=torch.float32).reshape(len(training_data), 3)\n",
    "y = tensor([i[1] for i in training_data], dtype=torch.float32).reshape(len(training_data), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data from disk, instead of using the generate_training_data function\n",
    "training_data = np.load(\"data/training_pendulum.npy\")\n",
    "X = tensor(training_data[:,:3], dtype=torch.float32)\n",
    "y = tensor(training_data[:,3], dtype=torch.float32).reshape(len(training_data),1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
