{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f83822-b831-4a2d-a338-5a7ccd7d4723",
   "metadata": {},
   "source": [
    "# Neural Networks with PyTorch: introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924685a0-150e-4eb8-b84e-c4deb8d8bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cf969-965d-459d-a552-a04751e5ddbf",
   "metadata": {},
   "source": [
    "## Brief introduction to PyTorch: tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90647cb-bdfb-4bdb-94f4-8d8c9274c45f",
   "metadata": {},
   "source": [
    "[PyTorch documentation](https://pytorch.org/docs/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0b8ea-7098-49f3-8110-279fd3b94e77",
   "metadata": {},
   "source": [
    "We can think of tensors as the PyTorch counterpart of NumPy arrays. Moreover, they offer a several advantages:\n",
    "- They can run on GPUs or other hardware accelerators.\n",
    "- They are optimized for automatic differentiation (backpropagation).\n",
    "- Tensors and NumPy arrays share the same underlying memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802289b-d1a3-49bb-87db-16a312c8308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from list to tensor\n",
    "x = [[1, 2],[3, 4]]\n",
    "x_tensor = torch.tensor(x)\n",
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6431f-9193-4fc1-9c92-c9eba0ac7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy array to tensor\n",
    "x_array = np.array(x)\n",
    "x_tensor = torch.from_numpy(x_array)\n",
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cc77c-d822-4f48-9ce2-06131e7c8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor of ones with the same shape as x_tensor\n",
    "x_ones = torch.ones_like(x_tensor)\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5c5d7-dc10-4699-8ee7-3f5ac0f235c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random tensor with the same shape as x_tensor\n",
    "x_rand = torch.rand_like(x_tensor, dtype=torch.float)\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135139f-f56f-49fe-960d-ea947c964dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tensors with given shape\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e442c6-61b1-4c65-b382-cde3d028c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor attributes\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "print(f\"Datatype: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98e2c2-d542-4322-a511-31b79a06db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.arange(12).reshape((3,4))\n",
    "print(f\"Tensor: {tensor}\")\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[:, -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63baf99a-e4f9-4300-acde-6f2b10a807bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate tensors\n",
    "conc_tensor = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "conc_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05e1b7-f793-4aa7-9494-89264971344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor multiplication\n",
    "t1 = tensor @ tensor.T\n",
    "# tensor multiplication (explicit formulation)\n",
    "t2 = tensor.matmul(tensor.T)\n",
    "print(t1,t2) \n",
    "\n",
    "# element-wise product \n",
    "e1 = tensor * tensor\n",
    "# element-wise product (explicit formulation)\n",
    "e2 = tensor.mul(tensor)\n",
    "print(e1,e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb94ad5-b63f-4c7f-86e2-be923e9e1370",
   "metadata": {},
   "source": [
    "### Interoperation with NumPy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4b3e2-4710-4850-a4eb-adf4190bacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensor to numpy arrays\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65f826-6dc1-4c4c-bd3a-2a5337e2c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the tensor will also change its numpy counterpart\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee15e-6fcb-4c36-9a6f-98dd05910be7",
   "metadata": {},
   "source": [
    "## Classification on the FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf873f80-7f5d-41ed-99ac-3f31c52248f5",
   "metadata": {},
   "source": [
    "In this section, we are going to see how to train a classifier on a famous dataset, called Fashion-MNIST. It consists of Zalandoâ€™s article images, represented by $28 \\times 28$ grayscale images.\n",
    "\n",
    "The following parameters are important for the loading functions:\n",
    "- `root`: is a string explicitating the path where the data are stored;\n",
    "- `train`: is a boolean specifying if the dataset is for training or for test;\n",
    "- `download`: is a boolean specifying to download the data (or not);\n",
    "- `transform`: specify the feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc1030-68df-41c6-bfbc-1c52aad28c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2015eac-7c8c-4429-a0cd-51b977ee01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 classes\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "# plot 9 random sample of the training set\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f077d49-674c-4436-8e1e-46d7c8d4376a",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac09cca-239f-45f5-b6f7-26dfe09ad8ee",
   "metadata": {},
   "source": [
    "For effective training, it is important to split the datasets into mini-batches and to\n",
    "shuffle them at each epoch. We can use the `DataLoader` class of PyTorch for this\n",
    "purpose and for iterating over batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d05a49f-4d39-4b98-82c1-e80a4a0a18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1c8cb-125a-41e1-b906-bdf7626097de",
   "metadata": {},
   "source": [
    "### Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb5ccf",
   "metadata": {},
   "source": [
    "We can define the architecture of the NN with a class derived from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b67078ac-6225-4e9d-a97b-6d1e84b50efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # convert a 28x28 tensor into a contiguous array of length 784\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Sequential is a container of different modules, such that data flows from one\n",
    "        # module to the next\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # linear combination with bias of inputs (array of length 784) into a\n",
    "            # array of length 512\n",
    "            nn.Linear(28*28, 512), \n",
    "            # ReLU activation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c550bf-7005-4314-a698-8724274b3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model (with random weights)\n",
    "model = NeuralNetwork()\n",
    "# print model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788b018-2ac1-4404-b614-5f04f0c39a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one random image\n",
    "X = torch.rand(1, 28, 28)\n",
    "# forward pass\n",
    "logits = model(X)\n",
    "\n",
    "# use Softmax to turn the outputs of the NN into probabilities\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# take the maximum probability class as the prediction\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa444d5-fdc2-4f30-a485-f59512920738",
   "metadata": {},
   "source": [
    "**Notice**: the cross-entropy loss in PyTorch (see\n",
    "[docs](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss))\n",
    "applies the Softmax function to the\n",
    "outputs of the network to get a tensor of probabilities before computing the loss, so we\n",
    "don't need to add a softmax operation to the final layer of the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd486ae-3551-4594-addc-6f5aada93374",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95777041-7ce1-4275-9ccb-8aca1b09d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Updating model parameters using gradient information\n",
    "        optimizer.step()\n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d662e96-9113-4056-97bf-d6d1e7726059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients \n",
    "    # are computed during test mode and also serves to reduce unnecessary \n",
    "    # gradient computations and memory usage for tensors with requires_grad = True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d776547-5cde-474f-a3da-4962c9103fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "# Main training loop\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_step(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_step(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2f3f4-8e38-44bc-88aa-7d7c3a2320d3",
   "metadata": {},
   "source": [
    "#### Bonus exercise:\n",
    "Solve the classification problem for the MNIST dataset. Use the\n",
    "`torchvision.datasets.MNIST` class to load the data. Plot the _average_ lossess on the\n",
    "training and the test sets vs epoch number. Check the predictions of the trained model\n",
    "on some samples of the test dataset. (No solution to this exercise is provided, as the\n",
    "implementation is very close to the one above :-))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
