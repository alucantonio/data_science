\documentclass{beamer}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{bm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Neural Networks}
\author{Prof. Alessandro Lucantonio}
\institute{Aarhus University - Department of Mechanical and Production Engineering}
\date{?/?/2023}

\begin{document}
	\frame{\titlepage}
	
	\begin{frame}
		\frametitle{Motivations}
		\begin{itemize}
			\setlength\itemsep{5mm}
			\item Neural Networks (NN) are one of the most flexible ML tools.
			\item Universal approximators!
			\item Can manipulate real and discrete data $\leadsto$ regression and classification problems.
			\item Not a single model: many type of NN (e.g. MLP, CNN, RNN).
			\item Inspired by biological systems.
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Perceptron - Visualization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.2]{images/perceptron}
			\caption{Representation of a perceptron.}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Perceptron - Formal}
		Notation: $n$ is as usual the number of features. $x$ is an input sample of length $n+1$ with $x_0 = 1$.
		
		\vspace{5mm}
		
		Perceptron:
		\begin{equation*}
			\begin{cases}
				n(\bm{x}) := \bm{w}^T \bm{x} = \sum_{j=0}^{n} w_j x_j,\\
				h(\bm{x}) := f(n(\bm{x})).
			\end{cases}
		\end{equation*}
	
		$n$ is the \textbf{net input} to the neuron.
		
		\vspace{5mm}
		
		$f$ is called \textbf{activation function}. Some common examples are:
		\begin{itemize}
			\item Linear:  $f(t) = at + b$.
			\item ReLU (Rectified Linear Unit): $f(t) := \max\{0, t\}$.
			\item Sigmoid: $f(t) := \frac{1}{1 + e^{-t}}$.
			\item Tanh (Hyperbolic tangent): $f(t) := \frac{e^{2t}-1}{e^{2t}+1}$
		\end{itemize}
		
	\end{frame}
	
	\begin{frame}
		\frametitle{Activation Functions - Visualization}
		\begin{figure}
			\centering
			\subfloat[Linear]{\includegraphics[scale=0.25]{images/linear}}
			\qquad
			\subfloat[ReLU]{\includegraphics[scale=0.25]{images/relu}}
			\\ \vspace{0.1cm}
			\subfloat[Sigmoid]{\includegraphics[scale=0.25]{images/sigmoid}}
			\qquad
			\subfloat[Tanh]{\includegraphics[scale=0.25]{images/tanh}}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{MultiLayer Perceptron (MLP) - Visualization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.23]{images/mlp}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{MLP - Formal}
		Notation: $\bm{a}^{(i)}$ is the input vector of the $i$-th layer and $\mathsf{W}^{(i)}$ is the weight matrix for each layer. $m$ is the number of layers.
		
		\vspace{5mm}
		
		Parenthesis: Why do we have a weight matrix for each layer? Each layer $i$ consists of $m_i$ neurons, hence necessarily has $m_i$ weight vectors, which we can arrange row-wise to form the matrix $\mathsf{W}^{(i)}$.
		
		\vspace{5mm} 
		
		For each layer $i$ compute
		\begin{equation*}
			\begin{cases}
				n_i(\bm{a}^{(i)}) := \mathsf{W}^{(i)} \bm{a}^{(i)},\\
				h_i(\bm{a}^{(i)}) := f_i(n_i(\bm{a}^{(i)})).
			\end{cases}
		\end{equation*}
	Compact form:
	\begin{equation*}
		h(\bm{x}) := f_{m-1}\big(\mathsf{W}^{(m-1)}f_{m-2}\big(\cdots f_1 \big(\mathsf{W}^{(1)} \bm{x}\big) \cdots\big)\big)
	\end{equation*}
	\end{frame}

	\begin{frame}
		\frametitle{MLP - Some comments}
		\begin{itemize}
			\setlength\itemsep{5mm}
			\item This kind of NN are also called \textbf{feedforward NN}, since we transfer information from input to output.
			\item The hypothesis function is non-convex! This is due to the fact that composition of convex functions is not necessarily convex. Hence, in the learning problem we will have many local minima and saddle points.
			\item Theory tells us that MLP with just $1$ hidden layer are universal approximators, in the sense that they approximate as well as you want "each" continuous function (not a formal statement, just to provide intuition).
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Tips and Tricks - Is one layer really enough?}
		Theory suggests us that the answer is yes, but pay attention. There exists cases for which an exponential number of units (w.r.t. the input dimension) are required to approximate well the data.
		
		\vspace{5mm}
		
		Introducing many layers helps to reduce the total number of units but complicate a bit the learning procedure (see later).
		
		\vspace{5mm}
		
		Then the question becomes: how to find the optimal number of layers? Validation is your best friend :)
		
		Include in your model selection process as many different architectures/activation functions as you can.
	\end{frame}

	\begin{frame}
		\frametitle{Back-propagation - Intuition}
		Notation: $E(\mathsf{W}) = \frac{1}{N} \sum_{j=0}^{N} E_j(\mathsf{W})$ and  $\mathsf{W} = (\mathsf{W}^{(1)}, \dots, \mathsf{W}^{(m-1)})$.
		
		\vspace{5mm}
		
		To apply gradient descent we have to compute $\nabla E(\mathsf{W})$. In the case of NN gradient descent algorithm is called \textbf{back-propagation}, since it allows the information from the cost to then ï¬‚ow backward through the network in order to compute the gradient.
		\begin{figure}
			\centering
			\includegraphics[scale=0.35]{images/backprop}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Back-propagation - Formal}
		Formal computation only for $E =$ MSE.
		
		\vspace{5mm}
		
		Goal: Our goal is to compute $\nabla E_i(\mathsf{W})$ or equivalently $\frac{\partial E_i}{\partial w_{jk}}$.
		
		\vspace{5mm}
		
		Chain rule:
		\begin{equation*}
			\frac{\partial E_i}{\partial w_{jk}} = \underbrace{\frac{\partial E_i}{\partial (n_t)_{j}}}_{A} \underbrace{\frac{\partial (n_t)_{j}}{\partial w_{jk}}}_{B}.
		\end{equation*}
		
		B:
		\begin{align*}
			\frac{\partial (n_t)_{j}}{\partial w_{jk}} = \frac{\partial}{\partial w_{jk}} \sum_{p} w_{jp} a^{(t)}_p =   a^{(t)}_k
		\end{align*}
	\end{frame}

	\begin{frame}
		\frametitle{Back-propagation - Formal}
		A:
		\begin{align*}
			\frac{\partial E_i}{\partial (n_t)_{j}} = \underbrace{\frac{\partial E_i}{\partial (h_t)_j}}_{C} \underbrace{\frac{\partial (h_t)_j}{\partial (n_t)_{j}}}_{D}
		\end{align*}
		D:
		\begin{align*}
			\frac{\partial (h_t)_j}{\partial (n_t)_j} = \frac{\partial}{\partial (n_t)_{j}} (f_t(n_t))_j = f_t((n_t)_j) = f'_t((n_t)_j) = (f'_t(n_t))_j
		\end{align*}
		C in the case of $t = m-1$ (output layer):
		\begin{align*}
			\frac{\partial E_i}{\partial (h_{m-1})_j} = \frac{\partial}{\partial (h_{m-1})_j} (\bm{y}_i - h_{m-1}(\bm{x}^i))^2 = -2((\bm{y}_i)_j - (h_{m-1}(\bm{x}^i))_j)
		\end{align*}
	\end{frame}

	\begin{frame}
		\frametitle{Back-propagation - Formal}
		
		
		C in the case of $t = m-2$ (notation):
		\begin{align*}
			\frac{\partial E_i}{\partial (h_{m-2})_j} = \sum_k \frac{\partial E_i}{\partial (n_{m-1})_k} \frac{\partial (n_{m-1})_k}{\partial (h_{m-2})_j}.
		\end{align*}
	
		Note that the terms $\frac{\partial E_i}{\partial (n_{m-1})_k}$ have been already calculated in the previous case. The other term is easy:
		\begin{align*}
			\frac{\partial (n_{m-1})_k}{\partial (h_{m-2})_j} = \frac{\partial}{\partial (h_{m-2})_j}\sum_{p} w_{kp} (h_{m-2})_p = {w}_{kj}.
		\end{align*}
		At this point we continue going backward and computing C for $t = m-3, \dots, 1$.
	\end{frame}

	\begin{frame}
		\frametitle{NN best practices}
		\begin{itemize}
			\item Initialize all weights randomly near zero, but \textbf{avoid} to initialize it constantly zero. As a general recipe: the more you are random the more you do not introduce bias.
			\item Try random starting configurations. For each configurations do multiple runs and look to the mean $\pm$ std results. 
			\item Number of epochs (i.e. number of iterations of SGD) are not an hyperparameter. Fix it to an appropriate number (e.g. 5000 or less) to avoid too slow convergences.
			\item Use regularization to counter overfitting.
		\end{itemize}
	\end{frame}
\end{document}