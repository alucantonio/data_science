\documentclass{beamer}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{bm}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\setbeamertemplate{footline}[frame number]
\title{Linear Regression}
\author{Prof. Alessandro Lucantonio}
\institute{Aarhus University - Department of Mechanical and Production Engineering}
\date{?/?/2023}

\begin{document}
	
	\frame{\titlepage}
	
	\section{Linear regression with one variable}

	\begin{frame}
		\frametitle{Weight-Height example}
		Dataset: heights and weights of different people.
		
		Task: build a model that predicts the height given the weight. 
		
		\begin{figure}
			\centering
			\includegraphics[scale=0.5]{images/linear_regression_data}
			\caption{Plot of the dataset}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{A solution - Linear Regression model}
		Remarks:
		\begin{itemize}
			\item Regression problem (continuous output).
			\item Data with different orders of magnitude.
		\end{itemize}
		A possible solution to this problem may be represented by a linear model (represented in red in the figure below). This learning algorithm is called \textbf{Linear Regression} (LR).
		\begin{figure}
			\centering
			\includegraphics[scale=0.4]{images/linear_regression_fit}
			\caption{Trained model (in red)}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{General ingredients}
		Notation:
		\begin{itemize}
			\item $x^i$: a data sample.
			\item $y^i$: the data target corresponding to $x$
			\item $N$: number of samples.
		\end{itemize}
	
		\vspace{5 mm}
		
		Model/hypothesis: $h_{\bm{w}}(x^i) = w_1x^i + w_0$, where $\bm{w} = [w_0, w_1]$ is the vector of parameter that has to be learned.
		
		In our example, $x^i$ is the weight of a single sample and $h_{\bm{w}}(x^i)$ corresponds to the prediction of its height. 
		
		\vspace{5 mm}
		
		The set $\mathcal{H}:= \{h_{\bm{w}}| \bm{w} \in \mathbb{R}^2\}$ is called \textbf{hypothesis space}.
		
		\vspace{5 mm}
		
		How to learn $\bm{w}$ from data?
		
	\end{frame}

	\begin{frame}
		\frametitle{Performance measure: Mean Squared Error (MSE)}
		To evaluate how good is the prediction we compute the error $(h_{\bm{w}}(x_i) - y_i)^2$.
		
		\vspace{5 mm}
		
		Notice that: $(h_{\bm{w}}(x_i) - y_i)^2 \geq 0$ and $(h_{\bm{w}}(x_i) - y_i)^2 = 0$ if and only if $h_{\bm{w}}(x_i) = y_i$. The \textbf{mean squared error} (MSE) is:
		$$E({\bm{w}}) := \frac{1}{N} \sum_{i=1}^{N} (h_{\bm{w}}(x_i) - y_i)^2.$$
		
		To find the parameters of the best model we minimize the MSE.
		
		$$\bm{w} \in \argmin_{\tilde{\bm{w}} \in \mathbb{R}^2} E(\tilde{\bm{w}}).$$
		
	\end{frame}

	\begin{frame}
		\frametitle{MSE - Visualization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.5]{images/mse}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{$n$-dimensional LR}
		\textsl{Dataset samples}.
		
		Previous case: $x^i \in \mathbb{R}, y^i \in \mathbb{R}$.
		
		\vspace{1 mm}
		
		Now:  $\bm{x}^i \in \mathbb{R}^n, y^i \in \mathbb{R}$.
		
		Notation: $x^i_j$ is the $j$-th coordinate of the $i$-th sample.
		
		\vspace{5 mm}
		
		\textsl{Candidate hypothesis}.
		
		\begin{align*}
			h_{\bm{w}}(\bm{x}) &= w_{n}x_n + w_{n-1}x_{n-1} + \dots + w_1 x_1 + w_0\\
			&= \sum_{i=0}^n w_i \tilde{x}_i = \bm{w}^T \tilde{\bm{x}},\\
		\end{align*}
		where $\bm{w} = [w_0, \dots, w_n]^T$ and $\tilde{\bm{x}} = [1, x_1, \dots, x_n]^T$.
		
	\end{frame}

	\begin{frame}
		\frametitle{n-dimensional LR}
		\textsl{MSE}.
		
		\begin{align*}
			E(\bm{w}) &= \frac{1}{N} \sum_{i=1}^{N} (h_w(\bm{x}^i) - y^i)^2\\
			&= \frac{1}{N} (\mathsf{X} \bm{w} - \bm{y})^T (\mathsf{X}\bm{w} - \bm{y})\\
			&= \frac{1}{N} ||\mathsf{X}\bm{w} - \bm{y}||^2
		\end{align*}
		
		where
		\begin{equation*}
			\mathsf{X} := \begin{bmatrix}
				\tilde{\bm{x}}^1\\
				\vdots\\
				\tilde{\bm{x}}^N 
			\end{bmatrix} \quad \bm{y} = \begin{bmatrix}
			y_1 \\
			\vdots\\
			y_N
		\end{bmatrix}.
		\end{equation*}
	In particular, $\mathsf{X} \in \mathbb{R}^{N \times (n+1)}$ and $y \in \mathbb{R}^N$.
	\end{frame}

	\begin{frame}
		\frametitle{Spot the minimum - Gradient descent}
		How to find $\bm{w} \in \argmin_{\tilde{\bm{w}} \in \mathbb{R}^2} E(\tilde{\bm{w}})$?
		
		\vspace{5mm}
		
		Main idea: geometrically, the gradient of a scalar function represents the direction of maximum slope. Hence, following the direction opposite to the gradient leads closer to the minimum of the function.
		
		\vspace{5mm}
		
		Formally:
		\begin{itemize}
			\item Start with an initial guess $\bm{w}^0$ (random).
			\item For $j \geq 0$, update $\bm{w}^{j+1} := \bm{w}^{j} + \bm{d}^j$, where $\bm{d}^j$ is such that
			\begin{equation*}
				E(\bm{w}^{j+1}) \leq E(\bm{w}^j) 
			\end{equation*}
		\end{itemize}
		Gradient descent: $\bm{d}^j = - \alpha \nabla E(\bm{w}^j)$; $\alpha>0$ is called \textbf{learning rate}.
	\end{frame}

	\begin{frame}
		\frametitle{Gradient descent - 3D visualization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.4]{images/gradient_descent_3D}
			\caption{In blue the global minimum, in red the iteration points.}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Gradient descent - 2D visualization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{images/gradient_descent_1}
			\caption{Learning rate = 0.1}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Gradient descent - 2D visualization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{images/gradient_descent_2}
			\caption{Learning rate = 0.4}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Gradient descent - 2D visualization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{images/gradient_descent_3}
			\caption{Learning rate = 0.5}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Batch, SGD and Mini-Batch - Intuition}
		Notation: $E(\bm{w}) = 1/N \sum_{i=1}^N E_i(\bm{w})$
		
		\vspace{5mm}
		
		
		The classical gradient descent update rule, i.e. update the weights vector computing the gradient of the entire cost function $E(\bm{w})$, is called \textbf{batch version}. However, for large number of samples ($N$) computing $\nabla E(\bm{w})$ is very time consuming.
		
		\vspace{5mm}
		
		To speed-up the update rule we approximate $\nabla E(\bm{w})$ with $\nabla E_i(\bm{w})$. This is the idea behind the so-called \textbf{Stochastic Gradient Descent} (SGD) or \textbf{online version}.
		
		\vspace{5mm}
		
		A trade-off between GD and SGD is called \textbf{mini-batch version}.
	\end{frame}

	\begin{frame}
		\frametitle{Batch, SGD and Mini-Batch - Formal}
		\textbf{Batch}
		\begin{itemize}
			\item Start with a random $\bm{w}^0$.
			\item For $j \geq 0$, update $\bm{w}^{j+1} := \bm{w}^{j} - \alpha \nabla E(\bm{w}^j)$.
		\end{itemize}
		
		\textbf{Stochastic Gradient Descent} (SGD or online)
		\begin{itemize}
			\item Start with a random $\bm{w}^0$.
			\item For $j \geq 0$ and for each pattern $1\leq i \leq N$ update $\bm{w}^{j+1} := \bm{w}^{j} - \alpha \nabla E_i(\bm{w}^j)$.
		\end{itemize}
	
		\textbf{Mini-Batch}. Fix an integer $1 \leq \text{mb} \leq N$(mini-batch size).
		\begin{itemize}
			\item Start with a random $\bm{w}^0$.
			\item For $j \geq 0$ and for each pattern $0 \leq i < \frac{N}{\text{mb}}$ update
			\begin{equation*}
				\bm{w}^{j+1} := \bm{w}^{j} - \alpha \nabla \sum_{k=i\cdot\text{mb} + 1}^{(i+1)\cdot\text{mb}}E_k(\bm{w}^j)
			\end{equation*}
		\end{itemize}
		
	\end{frame}

	\begin{frame}
		\frametitle{Tips and Tricks - How to choose?}
		\begin{itemize}
			\item Batch: usually more stable and provide a more accurate estimation of the gradient, but very slow.
			\item SGD: very fast, stochastic approximation of the gradient implies possible instability (Zig-zag effect)
			\item Mini-Batch: a trade-off (parallelism available).
		\end{itemize}
	In practice, through SGD and mini-batch we may not even reach the minimum, but it is enough to get close to it.
		\begin{figure}
			\centering
			\includegraphics[scale=0.35]{images/gd_mb_sgd}
			\caption{Batch vs SGD vs Mini-batch}
		\end{figure}
	
	\end{frame}

	\begin{frame}
		\frametitle{Gradient descent and normal equation for LR}
		We have $E(\bm{w}) = \frac{1}{N} ||\mathsf{X}\bm{w} - \bm{y}||^2$, hence
		\begin{equation*}
			\nabla E(\bm{w}) = \frac{1}{N} \nabla (||\mathsf{X}\bm{w} - \bm{y}||^2) = \frac{2}{N}\mathsf{X}^T(\mathsf{X}\bm{w} - \bm{y})
		\end{equation*}
	
	Normal equation ($\textcolor{red}{\iff}$ holds if $\mathsf{X}^T\mathsf{X}$ is invertible):
	\begin{align*}
		\nabla E(\bm{w}) = 0 &\iff \frac{2}{N}\mathsf{X}^T(\mathsf{X}\bm{w} - \bm{y}) = 0\\ 
		&\iff \mathsf{X}^T\mathsf{X}\bm{w} = \mathsf{X}^T\bm{y}\\
		& \textcolor{red}{\iff} \bm{w} = (\mathsf{X}^T\mathsf{X})^{-1}\mathsf{X}^T\bm{y}
	\end{align*}
	
	Gradient descent main iteration for LR:
	
	\begin{equation*}
		\bm{w}^{j+1} := \bm{w}^{j} - \frac{2\alpha}{N}\mathsf{X}^T(\mathsf{X}\bm{w}^j - \bm{y})
	\end{equation*}
	
		
	\end{frame}

	\begin{frame}
		\frametitle{Tips and Tricks - Invertibility of $\mathsf{X}^T\mathsf{X}$}
		
		Invertibility of $\mathsf{X}^T\mathsf{X} \iff$ columns of $X$ linearly independent (preprocessing information).
		
		\vspace{5mm}
		
		What happens when $\mathsf{X}^T\mathsf{X}$ is not invertible?
		
		\vspace{5mm}
		
		If two columns are linearly dependent, then those features are correlated (\textbf{redundant}).
		
		\vspace{5mm}
		
		Solution: discard one of those features.
	\end{frame}

	\begin{frame}
		\frametitle{Normal equation vs gradient descent}
		Normal equation:
		\begin{itemize}
			\item No hyperparameter (explicit solution).
			\item No need to iterate.
			\item $\mathcal{O}(N^3)$, since this is the cost to invert a dense matrix. In particular, it is slow when $N$ is large.
		\end{itemize}
	
		\vspace{5mm}
	
		Gradient descent:
		\begin{itemize}
			\item Need to choose the learning rate $\alpha$.
			\item Needs many iterations.
			\item $\mathcal{O}(N^2)$, hence faster when $N$ is large.
		\end{itemize}
	\end{frame}


	\begin{frame}
		\frametitle{Tips and Tricks - Standardization}
		General (not only for LR): features must be on a similar scale!
		\begin{itemize}
			\item Speed up the convergence of gradient descent.
			\item Try to have (on average) $-1 \leq x^i \leq 1$.
		\end{itemize}
		Common techniques:
		\begin{itemize}
			\item \textbf{Feature scaling}. Compute the max $\bm{M} := [\max_i x^i_j]_j$ and the min $\bm{m} := [\min_i x^i_j]_j$ data value. Then normalize each feature as follows
			\begin{equation*}
				\bm{x}_{\text{norm}}^i = \frac{\bm{x}^i - \bm{m}}{\bm{M} - \bm{m}}
			\end{equation*} 
			\item \textbf{Mean normalization}. Compute mean $\bm{\mu}$ $(\mu_j := \mathbb{E}[[x^i_j]_i])$ and standard deviation $\bm{\sigma}$ $(\sigma_j := \sqrt{\text{Var}[[x^i_j]_i]})$ of the data. Then normalize each feature as follows
			\begin{equation*}
				\bm{x}_{\text{norm}}^i = \frac{\bm{x}^i -\bm{\mu}}{\bm{\sigma}} 
			\end{equation*}
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Tips and Tricks - Standardization}
		\begin{figure}
			\centering
			\includegraphics[scale=0.35]{images/feature-scaling}
		\end{figure}

		
	\end{frame}



	\begin{frame}
		\frametitle{Polynomial regression (PR)}
		PR corresponds to polynomial hypothesis, i.e. of the form
		\begin{equation*}
			h_{\bm{w}}(\bm{x}) = \sum_{j=0}^n w_j x^j_j.
		\end{equation*}
		
		\vspace{5mm}
		More in general: linear basis expansion (LBE)
		\begin{equation*}
			h_{\bm{w}}(\bm{x}) = \sum_{j=0}^n w_j \phi_j(\bm{x}),
		\end{equation*}
		where $\phi_j: \mathbb{R}^n \rightarrow \mathbb{R}$.
	\end{frame}

	\begin{frame}
		\frametitle{Underfitting and overfitting - main intuition}
		
		Imagine that you have to prepare an exam. 
		
		\vspace{5mm}
		
		Doing only a few exercises lead a poor perfomance both on homeworks and on the exam exercises. This is called \textsl{underfitting}: you have a bad performance on the exam because you did not trained enough.
		
		\vspace{5mm}
		
		Moreover, brutally memorize all the homework lead to a perfect score on homeworks (trivially) but probably a bad score on the exam exercises. This is called \textsl{overfitting}: you have a bad performance on the exam because you did not captured the true essence of your homework, you have also memorize their ''noise'' (homework pecularities).
	\end{frame}

	\begin{frame}
		\frametitle{Underfitting and overfitting - another example}
		\begin{figure}
			\centering
			\includegraphics[scale=0.35]{images/overfitting_poly}
			\caption{Underfitting (degree 1), good fitting (degree 4), overfitting (degree 15).}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{How to counter overfitting}
		
		Overfitting can be countered in many ways.
		
		\vspace{5mm}
		
		\begin{itemize}
			\item Validation, the true core of Machine Learning.
			\item Early stopping.
			\item Ensembling.
			\item Regularization.
		\end{itemize}
	
		\vspace{5mm}
	
		Now we focus on regularization and the general intuition behind it, lately in this course we are going to address the other points.
	\end{frame}

	\begin{frame}
		\frametitle{Tikhonov regularization}
		
		Overfitting phenomenon is highly correlated with complex models. To avoid complex models, the idea is to penalize models with large weights. One way to do it is to consider the following cost function
		
	\vspace{5mm}
	
		\begin{equation*}
			E_{r}(\bm{w}) = E(\bm{w}) + \underbrace{\lambda ||\bm{w}||^2}_{R_{\lambda}(\bm{w})}.
		\end{equation*}
		
	\vspace{5mm}
	
	$R_{\lambda}$ is called \textsl{Tikhonov regularization} (or $L^2$ regularization). More in general, we will call \textsl{regularization term} the term added to the error function involving the type of regularization chosen.
	
	\vspace{5mm}
	
	$\lambda > 0$ is an hyperparameter that must be chosen in the model selection phase.
	\end{frame}

	\begin{frame}
		\frametitle{LR with Tikhonov regularization}
		New cost function:
		\begin{equation*}
			E_{r}(\bm{w}) = E(\bm{w}) + \lambda ||\bm{w}||^2.
		\end{equation*}
		
		Gradient of the cost function:
		\begin{equation*}
			\nabla E_r(\bm{w}) = \nabla E(\bm{w}) + 2 \lambda \bm{w} = 2(\frac{1}{N}\mathsf{X}^T(\mathsf{X}\bm{w} - \bm{y}) + \lambda \bm{w})
		\end{equation*}
	
		Normal equation:
		\begin{equation*}
			\bm{w} = (\mathsf{X}^T\mathsf{X} + \lambda \mathsf{I})^{-1}\mathsf{X}^T\bm{y}
		\end{equation*}
	
	Note that in this case $\mathsf{X}^T\mathsf{X} + \lambda \mathsf{I}$ is \textsl{always} invertible (why? :) )
	\end{frame}

	\begin{frame}
		\frametitle{Most common regularization techniques}
		\begin{itemize}
			\item Tikhonov regularization: $R_{\lambda}(\bm{w}) = \lambda ||w||_2$. Tends to bring all the weights to small values.
			\item Lasso: $R_{\lambda}(\bm{w}) = \lambda ||w||_1$. Tends to bring some weights to $0$ (feature selection).
			\item Elastic net: $R_{\lambda}(\bm{w}) = \lambda_1 ||w||_1 + \lambda_2 ||w||_2$.
		\end{itemize}
	\end{frame}
\end{document}