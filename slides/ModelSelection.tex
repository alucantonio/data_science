\documentclass{beamer}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{bm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\setbeamertemplate{footline}[frame number]
\title{Regularization and Model Selection}
\author{Prof. Alessandro Lucantonio}
\institute{Aarhus University}
\date{}

\setbeamertemplate{navigation symbols}{}

\begin{document}
	\frame{\titlepage}
	
		\begin{frame}
		\frametitle{Underfitting and overfitting - intuition}
		
		Imagine that you have to study for a written exam consisting of exercises. 
		
		\vspace{5mm}
		
		Practicing on only a few exercises leads to poor perfomance both on homework and at the exam. This is called \textsl{underfitting}: the performance at the exam will be bad because of poor training.
		
		\vspace{5mm}
		
		Moreover, memorizing all the solutions to homework leads to the maximum score on homework (trivially) but probably a bad score on the exam exercises. This is called \textsl{overfitting}. The bad performance on the exam occurs because  you did not capture the true essence of the homework, rather you have memorized their peculiarities.
	\end{frame}
	
	\begin{frame}
		\frametitle{Underfitting and overfitting - analytical example}
		\begin{figure}
			\centering
			\includegraphics[scale=0.35]{images/overfitting_poly}
			\caption{Underfitting (polynomial degree 1), good fitting (polynomial degree 4), overfitting (polynomial degree 15).}
		\end{figure}
		
		Overfitting can be countered through:
		\begin{itemize}
			\item regularization;
			\item validation.
		\end{itemize}
	\end{frame}
	
	
	\begin{frame}
		\frametitle{Regularization}
		
		Overfitting is correlated with ``complex" models: to avoid them, we penalize models with large weights. One way to do this is to consider the following cost function
		\begin{equation*}
			E_{r}(\bm{w}) = E(\bm{w}) + R_{\lambda}(\bm{w})
		\end{equation*}
		where $R_{\lambda}$ is the \textsl{regularization term}. $\lambda > 0$ is a hyperparameter that must be chosen in the \textbf{model selection phase} (see later).
		
		\vspace{5mm}
		
		Most common regularization techniques (do \textbf{not} include bias term):
		\begin{itemize}
			\item Tikhonov (or $L^2$ or \textit{Ridge}): $R_{\lambda}(\bm{w}) = \lambda ||\bm{w}||^2_2 = \lambda \sum_i w_i^2$. Tends to make \textit{all} weights small.
			\item LASSO (or $L^1$): $R_{\lambda}(\bm{w}) = \lambda ||\bm{w}||_1 = \lambda \sum_i |w_i|$. Tends to make \textit{some} weights $0$ (feature selection).
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Linear Regression with Tikhonov regularization}
		\begin{itemize}
			\item Cost function (without regularization):
			\begin{equation*}
				E(\bm{w}) = \frac{1}{N} ||\mathsf{X} \bm{w} - \bm{y}||^2
			\end{equation*}
			\item Regularized cost function:
			\begin{equation*}
				E_{r}(\bm{w}) = E(\bm{w}) + \lambda ||\bm{w}||^2
			\end{equation*}
			\item Gradient of the regularized cost function:
			\begin{equation*}
				\nabla E_r(\bm{w}) = \nabla E(\bm{w}) + 2 \lambda \bm{w} = 2\left(\frac{1}{N}\mathsf{X}^T(\mathsf{X}\bm{w} - \bm{y}) + \lambda \bm{w}\right)
			\end{equation*}
			\item Normal equation:
			\begin{equation*}
				\bm{w} = (\mathsf{X}^T\mathsf{X} + \lambda \mathsf{I})^{-1}\mathsf{X}^T\bm{y}
			\end{equation*}
			Note that in this case $\mathsf{X}^T\mathsf{X} + \lambda \mathsf{I}$ is \textsl{always} invertible (why?).
		\end{itemize}
		
		
	\end{frame}
	
\begin{frame}
	\frametitle{Bias-Variance trade-off}
	\textbf{Bias}: expected discrepancy between targets and predictions given by the model.
	
	\vspace{5mm}
	
	\textbf{Variance}: measure of the deviation from the expected value given by the model that any particular sampling of the data (from the same underlying data-generating distribution) is likely to cause.
	
	\vspace{5mm}
	
	Some intuitions on the effect of regularization:
	\begin{itemize}
		\item High $\lambda$ $\leadsto$ simple model $\leadsto$ high bias (underfitting).
		\item Low $\lambda$ $\leadsto$  complex model $\leadsto$  high variance (overfitting).
		\item Intermediate $\lambda$: optimal solution (good bias-variance trade-off).
	\end{itemize}
%	\begin{figure}
%		\centering
%		\includegraphics[scale=0.3]{images/bias-variance}
%		%\caption{Yellow curve: validation error.}
%	\end{figure}
\end{frame}
	
	\begin{frame}
		\frametitle{Generalization}
		Central challenge in Machine Learning: \textbf{generalization}! Our algorithm must perform well on \textit{new, previously unseen} inputs.
		
		\vspace{5mm}
		
		Recall that we have to find a balance between bias and variance. Even if we limit the complexity of our model using regularization, the training set does not provide a good estimate of the test (generalization) error.
		
		\vspace{5mm}
		
		In other words, generalization is compromised if we choose hyperparameters (including the regularization factor) only according to the training error.
	\end{frame}

	\begin{frame}
	\frametitle{Generalization - Training vs test error}
	\begin{figure}
		\centering
		\includegraphics[scale=0.8]{images/model_selection_general_idea}
		\caption{Training (blue) and test (red) errors as the model complexity varies.}
	\end{figure}
\end{frame}

	

	\begin{frame}
		\frametitle{Model selection and model assessment}
		
		In general, we should distinguish between two phases:
		\begin{enumerate}
			\item Model selection: estimate the performance of different models trained with different hyperparameters. 
			\item Model assessment: after choosing a final model we evaluate its performance on \textsl{new, previously unseen} (test) data.
		\end{enumerate}
		
		\vspace{5mm}
		
		\begin{itemize}
			\item For step 1: do \textbf{not} use the \textit{test set} to tune hyperparameters; use the \textbf{validation set}. The test set must never be touched until model assessment.
			\item For step 2: once we have selected a model using the validation set, we can assess its \textit{generalization error} on the \textbf{test set}. %Splitting the dataset into training and validation sets is called \textit{hold-out}.
		\end{itemize}
		
	\end{frame}

	\begin{frame}
		\frametitle{Double Hold-out - model selection/assessment workflow}
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{images/hold-out}
		\end{figure}
		\begin{enumerate}
			\item Split the entire dataset in three sets: training, validation and test (typically 60\%-20\%-20\% or  70\%-20\%-10\% of the total dataset). 
			\item Fit the model on the training set using different hyperparameters until a model is found that \textbf{maximizes the validation performance}. \textit{Optional}:  re-train the best model using training+validation. 
			\item Assess the generalization capability of the best (re-trained) model by evaluating its performance on the test set.
		\end{enumerate}	
	\end{frame}



	\begin{frame}
		\frametitle{Cross-Validation}
		\begin{columns}
			\column{0.38\linewidth}
			\begin{figure}
				\centering
				\includegraphics[scale=0.4]{images/cross-validation}
				\caption{5-fold CV}
			\end{figure}
			\column{0.65\linewidth}
			\begin{enumerate}
				\small
				\item Hold out the test data.
				\item Split the remaining data (development set) in $k$ disjoint \textit{folds} ($k$-fold CV).
				\item Use $k-1$ folds as the training set and the remaining fold as the validation set. Compute the performance measure (MSE, accuracy\dots) over the validation set. Repeat $k$ times by changing the training set (see figure). 
				\item Compute the \textbf{validation performance} as the mean $\pm$ standard deviation of the performance measure across the $k$ runs.
			\end{enumerate}
		\end{columns}
		
		\vspace{5mm}
		
		\textit{Pro}: Not sensitive to a particular partition of the data.
		
		\textit{Con}: Computationally expensive (but parallellizable).
		
	\end{frame}

	\begin{frame}
		\frametitle{Workflow for selection using CV and assessment}
		\begin{enumerate}
			\item Split dataset into two sets: development (model selection) and test (model assessment).
			\item Use the cross validation method on the development set to select the best model, i.e. the one that \textbf{maximizes the mean validation performance}.
			\textit{Optional}:  retrain the best model using the entire development set. 
			\item Evaluate the generalization error on the test set.
		\end{enumerate}
	
	\end{frame}

	\begin{frame}
		\frametitle{Grid search}
		
		How to search for the best set of hyperparameters?
		
		\vspace{1mm}
		
		\textbf{Example}: two hyperparameters (learning rate $\alpha$ and regularization factor $\lambda$). 
		\begin{enumerate}
			\item Consider sets of three values for each parameter, e.g. $\alpha_{\text{vals}} = \{0.001, 0.01, 0.1\}, \lambda_{\text{vals}} = \{0.0001, 0.001, 0.01\}$. 
			
			Evaluate the model with $(\alpha, \lambda) = (i,j)$ for $i \in \alpha_{\text{vals}}, j \in \lambda_{\text{vals}}$.
			
			\begin{figure}
				\centering
				\includegraphics[scale=0.35]{images/grid-search}
			\end{figure}
			
			\item Choose the pair corresponding to the \textbf{best performance on the validation set} based, for example, on a $k$-fold CV or double hold-out. 
			
			\item \textbf{Refinement}: Suppose that the best pair is $(0.1, 0.001)$. Then we can ``zoom in" and do another grid search with e.g. $\alpha_{\text{vals}} = \{0.075, 0.1, 0.125\}, \lambda_{\text{vals}} = \{0.00075, 0.001, 0.00125\}$. 
		\end{enumerate}
		
		
	\end{frame}
	
	\begin{frame}
		\frametitle{Early stopping}
		\begin{itemize}
			\item Store a copy of the model weights every epoch the validation loss improves;
			\item if the validation loss does not improve for a fixed number of epochs (\textit{patience}), terminate and return the best weights.
		\end{itemize}
		\begin{figure}
			\centering
			\includegraphics[scale=0.45]{images/early_stopping}
			\caption{Learning curves for a classifier.}
		\end{figure}
		
		\textit{Notice}: the training loss should include the regularization term, but the validation loss should not.
	\end{frame}
	
%	\begin{frame}
%		\frametitle{Effect of the training set size}
%		\begin{itemize}
%			\item With high \textbf{bias}: with large training set, training and validation errors will be high and close to each other. Increasing training set size (alone) will not help to reduce the validation error. 
%			\item With high \textbf{variance}:  with large training set, training error will increase while validation error will decrease without leveling off. Also, training error $<$ validation error but their difference remains significant. Increasing training set size is likely to help to improve the validation error.
%		\end{itemize}
%	\end{frame}

	
\end{document}